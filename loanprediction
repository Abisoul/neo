#importing libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.metrics import classification_report, confusion_matrix
import warnings
warnings.filterwarnings("ignore")


#importing the datasets
traindata = pd.read_csv('train_set.csv')
testdata = pd.read_csv('test_set.csv')

#Summary of the datasets
traindata.shape #Size of training dataset
testdata.shape #Size of test dataset

traindata.info() #a view of variables
testdata.info() #a view of variables

traindata.head(5)

#merging the two datasets
loandata = pd.concat([traindata,testdata])

#checking for missing values
loandata.isnull().sum()

# DATASET CLEANING AND PREPROCESSING
# treating the missing values: replacing with the column's mode
loandata['Gender'].fillna(loandata['Gender'].mode()[0], inplace = True)
loandata['Married'].fillna(loandata['Married'].mode()[0], inplace = True)
loandata['Dependents'].fillna(loandata['Dependents'].mode()[0], inplace = True)
loandata['Self_Employed'].fillna(loandata['Self_Employed'].mode()[0], inplace = True)
loandata['Credit_History'].fillna(loandata['Credit_History'].mode()[0], inplace = True)

# treating the missing values: replacing with the column's mean
loandata["LoanAmount"].fillna(loandata["LoanAmount"].mean(skipna=True), inplace=True)
loandata['Loan_Amount_Term'].fillna(loandata['Loan_Amount_Term'].value_counts().idxmax(), inplace=True)

############ Count number of Categorical and Numerical Columns ######################
categorical_columns = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area','Credit_History','Loan_Amount_Term']
#categorical_columns = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area','Loan_Amount_Term']

print(categorical_columns)
numerical_columns = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']
print(numerical_columns)


# EXPLORATORY DATA ANALYSIS 
### Data Visualization of the columns
#Target variable
df =  pd.read_csv('train_set.csv')
data = df["Loan_Status"].value_counts()
labels = df["Loan_Status"].unique().tolist()
colors = ["#1f77b4", "#ff7f0e"]
explode = (0.1, 0)  
plt.pie(data, labels=labels, explode=explode, colors=colors,
autopct='%1.1f%%', shadow=True, startangle=140)
plt.title("Pie Chart of Loan_Status")
plt.show()

#Univariate Analysis of Categorical variables
fig,axes = plt.subplots(4,2,figsize=(12,15))
for idx,cat_col in enumerate(categorical_columns):
    row,col = idx//2,idx%2
    sns.countplot(x=cat_col,data=loandata,hue='Loan_Status',ax=axes[row,col])


plt.subplots_adjust(hspace=1)

#Numerical
fig,axes = plt.subplots(1,3,figsize=(17,5))
for idx,cat_col in enumerate(numerical_columns):
    sns.boxplot(y=cat_col,data=loandata,x='Loan_Status',ax=axes[idx])

#print(loandata[numerical_columns].describe())
plt.subplots_adjust(hspace=1)
#checking for missing values again
loandata.isnull().sum()

#drop insignificant variable
loandata.drop('Loan_ID', inplace=True, axis='columns')

#violin plot to show distribution of ApplicantIncome

df1 = df['ApplicantIncome']
amount = pd.DataFrame(df1)

plt.figure(figsize=(10,5))
plt.title('Income distribution of Applicants')
#sns.distplot(amount['ApplicantIncome']);
sns.violinplot(amount['ApplicantIncome']);


#data transformation
loandata['Gender'] = loandata['Gender'].replace({"Female": 0, "Male": 1})
loandata['Married'] = loandata['Married'].replace({'No' : 0,'Yes' : 1})
loandata['Dependents'] = loandata['Dependents'].replace({'0':0,'1':1,'2':2,'3+':3})
loandata['Education'] = loandata['Education'].replace({'Not Graduate' : 0, 'Graduate' : 1})
loandata['Self_Employed'] = loandata['Self_Employed'].replace({'No' : 0,'Yes' : 1})
loandata['Property_Area'] = loandata['Property_Area'].replace({'Semiurban' : 0, 'Urban' : 1,'Rural' : 2})



# Splitting the dataset into train and test in the same ratio as before cleaning.
train = loandata.iloc[:614]
test = loandata.iloc[614:]


#dropping the last column for the test set 
#test = test.drop('Loan_Status', inplace=True, axis='columns')
#test.drop('Loan_Status', inplace=True, axis='columns')

# dependent variable transformation
train['Loan_Status'] = train['Loan_Status'].replace({'N': 0,'Y': 1})


# Correlation Analysis : heatmap
plt.figure(figsize = (10,10))
corr_mat = train.corr()
sns.heatmap(corr_mat, annot=True, cmap='RdYlBu')

#boxplots
sns.set_theme(style="whitegrid")
sns.boxplot(x=train["LoanAmount"]).set(title='Boxplot of Loan Amount')
sns.set_theme(style="whitegrid")
sns.boxplot(x=train["ApplicantIncome"]).set(title='Boxplot of Income')

#scatterplot
sns.scatterplot(data = train, x = "LoanAmount", y = "ApplicantIncome").set(title='Relationship Between Loan_Amount and Income')

plt.show()

#BULIDING THE MODELS
# separating input variables and Target Variable from the train data.
#train['Loan_Status'] = train['Loan_Status'].replace({'N': 0,'Y': 1})
x = train.drop('Loan_Status', axis='columns')
y = train['Loan_Status']


xtest = test.drop('Loan_Status', axis='columns')
ytest = train['Loan_Status'].iloc[:367]
#x = train.drop('Loan_ID', axis='columns')

#Decision Trees
from sklearn.tree import DecisionTreeClassifier

dtree = DecisionTreeClassifier()
dtree.fit(x,y)

# Now we will be making the predictions on the testing data directly as it is of more importance.
from sklearn import metrics

# Getting the accuracy score for Decision Tree
dtree_pred = dtree.predict(xtest) #prediction with DT model
print("Accuracy Score =", format(metrics.accuracy_score(ytest,dtree_pred)))

# Classification report and confusion matrix of the decision tree model
print(confusion_matrix(ytest, dtree_pred))
print(classification_report(ytest,dtree_pred))
sns.heatmap(confusion_matrix(ytest, dtree_pred), annot = True)

    
# Getting feature importances for Decision Tree model
(pd.Series(dtree.feature_importances_, index=x.columns).plot(kind='barh',title= 'Feature Importance of the DecisionTree model'))


#LogisticRegression
from sklearn.linear_model import LogisticRegression

lreg = LogisticRegression()
lreg.fit(x,y)


lreg_pred = lreg.predict(xtest) #prediction with LG model
print("Accuracy_Score =", format(metrics.accuracy_score(ytest, lreg_pred)))

# Classification report and confusion matrix 
print(confusion_matrix(ytest,lreg_pred))
print(classification_report(ytest,lreg_pred))
sns.heatmap(confusion_matrix(ytest, lreg_pred), annot = True)



importance = lreg.coef_[0]

for i,j in enumerate(importance):
	print('Feature: %0d, Score: %.3f' % (i,j))
   # print('Feature: %s, Score: %.3f' % (train.feature_names[i],j))
    

plt.barh([X for X in range(len(importance))], importance)
plt.title('Feature Importance of the LogisticRegression model')
plt.show()

# RANDOM FOREST
from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier(n_estimators=200)
rfc.fit(x, y)

# Getting the accuracy score for Random Forest
rfc_pred = rfc.predict(xtest)
print("Accuracy_Score =", format(metrics.accuracy_score(ytest, rfc_pred)))

print(confusion_matrix(ytest, rfc_pred))
print(classification_report(ytest,rfc_pred))
sns.heatmap(confusion_matrix(ytest, rfc_pred), annot = True)

# Getting feature importances for SVM model
from sklearn.inspection import permutation_importance
#one_hot_xtest.reindex(columns=one_hot_x, axis="columns").

perm_importance1 = permutation_importance(lreg,xtest, ytest)

features_names1 = ['Gender', 'Married','Dependents','Education','Self_Employed','ApplicantIncome','CoapplicantIncome','LoanAmount','Loan_Amount_Term','Credit_History','Property_Area']
features1 = np.array(features_names1)
#xtest_reindexed = one_hot_xtest[one_hot_x]
sorted_idx1 = perm_importance1.importances_mean.argsort()
plt.barh(features1[sorted_idx1], perm_importance1.importances_mean[sorted_idx1])
plt.xlabel("Feature Importance")
plt.title('Feature Importance of the LogisticRegression model')


#Bar chart of Recall values
data = {"Classifier":["DecisionTree", "LogisticRegression", "Random Forest"],

        "Recall":[0.69,0.85,0.81]

        };
c = ['red', 'yellow', 'blue']
dataFrame = pd.DataFrame(data=data);
# Draw a bar chart
dataFrame.plot.bar(x="Classifier", y="Recall", rot=70, color = c, title="Classification Performances by Recall");
plt.show(block=True);


#Bar chart of Performances
data = {"Classifier":["DecisionTree", "LogisticRegression", "RandomForest"],

        "accuracyScore":[60,65,63]

        };
c = ['black', 'green', 'orange']
dataFrame = pd.DataFrame(data=data);
# Draw a bar chart

dataFrame.plot.barh(x="Classifier", y="accuracyScore", color = c, rot=70, title="Classification Performances by Accuracy");
plt.show(block=True);


#Bar chart of F1 values
data = {"Classifier":["DecisionTree", "LogisticRegression", "Random Forest"],

        "F1Score":[0.70,0.77,0.75]

        };
c = ['red', 'yellow', 'blue']
dataFrame = pd.DataFrame(data=data);
# Draw a bar chart
dataFrame.plot.bar(x="Classifier", y="F1Score", rot=70, color = c, title="Classification Performances by F1 Score");
plt.show(block=True);


#computing SHAP values

from probatus.interpret import ShapModelInterpreter

# SHAP feature importance and summary plot for DT model
dtshap = dtree.fit(x,y)
shap_interpreter = ShapModelInterpreter(dtshap)
DT_importance = shap_interpreter.fit_compute(x, xtest, y, ytest, approximate=False)
DT_importance

DT_shap_plot = shap_interpreter.plot('importance')

DT_summary = shap_interpreter.plot('summary') #summary plot

# SHAP feature importance and summary plot for LR model
lgshap = lreg.fit(x,y)
lgshap_interpreter = ShapModelInterpreter(lgshap)
LG_importance = lgshap_interpreter.fit_compute(x, xtest, y, ytest, approximate=False)
LG_importance

LG_shap_plot = lgshap_interpreter.plot('importance', title = "SHAP Feature Importance of the SVM model")
LG_summary = lgshap_interpreter.plot('summary') #summary plot



# SHAP feature importance and summary plot for RF model
rfcshap = rfc.fit(x,y)
rfcshap_interpreter = ShapModelInterpreter(rfcshap)
RFC_importance = rfcshap_interpreter.fit_compute(x, xtest, y, ytest, approximate=False)
RFC_importance

RFC_shap_plot = rfcshap_interpreter.plot('importance', title = "SHAP Feature Importance of the RandomForest model")
RFC_summary = rfcshap_interpreter.plot('summary') #summary plot
