# -*- coding: utf-8 -*-
"""Exploratory.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Bbpf_sTsNbqPQX3yaPvdP2qYEuh66nDn
"""

# -*- coding: utf-8 -*-
"""basic_eda_spark.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/besherh/BigDataManagement/blob/main/SparkNotebooks/basic_eda_spark.ipynb

#Setting up PySpark in Colab
Spark is written in the Scala programming language and requires the Java Virtual Machine (JVM) to run. Therefore, our first task is to download Java.
"""

!apt-get install openjdk-8-jdk-headless

"""Next, we will install Apache Spark 3.0.1 with Hadoop 2.7 .

"""

!wget https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop2.7.tgz

"""Now, we just need to unzip that folder.

"""

!tar xf /content/spark-3.2.1-bin-hadoop2.7

"""There is one last thing that we need to install and that is the findspark library. It will locate Spark on the system and import it as a regular library.


"""

!pip install -q findspark

"""Now that we have installed all the necessary dependencies in Colab, it is time to set the environment path. This will enable us to run Pyspark in the Colab environment.

"""

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.2.1-bin-hadoop2.7"

"""We need to locate Spark in the system. For that, we import findspark and use the findspark.init() method."""

import findspark
findspark.init()
findspark.find()

"""Now, we can import SparkSession from pyspark.sql and create a SparkSession, which is the entry point to Spark.

You can give a name to the session using appName() and add some configurations with config() if you wish.
"""

from pyspark.sql import SparkSession

spark = SparkSession.builder\
        .master("local")\
        .appName("MyFirstEDA")\
        .config('spark.ui.port', '4050')\
        .getOrCreate()

"""Finally, print the SparkSession variable."""

spark

#then upload the csv files it to your colab session
dfad = spark.read.csv("/content/ad-clicks.csv", header=True, inferSchema=True)

dfbuy = spark.read.csv("/content/buy-clicks.csv", header=True, inferSchema=True)

dfgame = spark.read.csv("/content/game-clicks.csv", header=True, inferSchema=True)

df_level= spark.read.csv("/content/level-events.csv", header=True, inferSchema=True)

df_team_assignment = spark.read.csv("/content/team-assignments.csv", header=True, inferSchema=True)

df_team = spark.read.csv("/content/team.csv", header=True, inferSchema=True)

df_user_session = spark.read.csv("/content/user-session.csv", header=True, inferSchema=True)

df_user = spark.read.csv("/content/users.csv", header=True, inferSchema=True)

df_user = spark.read.csv("/content/combined-data.csv", header=True, inferSchema=True)

#Show column details
#The first step in an exploratory data analysis is to check out the schema of the dataframe. This will give you an overview of the columns in the dataframe along with their data types.

dfad.printSchema()
#you can use df.columns to check the columns without datatypes!

#Display Rows

dfad.show(5)

"""#Print summary statistics.
 We can print the summary statistics for all the columns using the describe() method:
"""

dfad.describe()

"""as you can see the output is not very clear, let's try to transpose the output (rows to columns)"""

dfad.describe().toPandas().transpose()

###Bar plot for Ad Category
import matplotlib.pyplot as plt
import seaborn as sns

dfad = dfad.groupby("adCategory").count().toPandas()

display(dfad)

plt.figure(figsize=(10,5))
plt.title('Ad Category clicked on in session')
sns.barplot(dfad['adCategory'],dfad['count'])

# Pie plot of Ad Categories
plt.figure(figsize=(15,10))
plt.title('Ad Category clicked on in session')
plt.pie(dfad['count'],labels=dfad['adCategory'],autopct='%.0f%%')

# Visualisation of Devices used
df_userd =df_user_session.groupby('platformType').count().toPandas()

display(df_userd)

plt.figure(figsize=(10,5))
plt.title('Devices used in the session')
sns.barplot(df_userd['platformType'],df_userd['count']);

plt.figure(figsize=(15,10))
plt.title('Devices used in the session')
plt.pie(df_userd['count'],labels=df_userd['platformType'],autopct='%.0f%%')

df_level= df_level.toPandas()
team_level = df_level.groupby("teamLevel").agg({'teamId': "sum"})
team_level.plot.bar(title="Chart of Team Levels",color='r')

dfbuy.printSchema()
dfbuy.limit(3).toPandas()

ranked_dfbuy = dfbuy
ranked_dfbuy.printSchema()

team_level

from pyspark.sql.functions import sum, col, desc
df = ranked_dfbuy.groupBy("team") \
  .agg(sum("price").alias("sum_price")) \
  .filter(col("sum_price") > 0)  \
  .sort(desc("sum_price")) \
  .show(10)

ranked_dfbuy.groupBy("team").sum("price").show()

dfGroup=ranked_dfbuy.groupBy("team") \
          .agg(sum("price").alias("sum_price"))

dfSort = dfGroup.sort(desc("sum_price"))

dfsort = dfSort.limit(10).toPandas()

display(dfsort)

plt.figure(figsize=(10,5))
plt.title('Top 10 teams based on highest purchase')
sns.barplot(dfsort['team'],dfsort['sum_price']);

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

pandas_dfbuy = dfbuy.toPandas()

plt.figure(figsize=(10,5))
plt.title('Price distribution of items purchased')
#sns.distplot(pandas_dfbuy['price']);
sns.violinplot(pandas_dfbuy['price']);

pandas_dfbuyp = dfbuy.groupBy('price').count().toPandas()
display(pd)
plt.figure(figsize=(10,5))
plt.title('Product price distribution')
sns.barplot(pandas_dfbuyp['price'],pandas_dfbuyp['count']);

pandas_dfbuyp

pandas_dfbuyp = dfbuy.groupBy('buyId').count().toPandas()
display(pd)
plt.figure(figsize=(10,5))
plt.title('Product with highest purchase')
sns.barplot(pandas_dfbuyp['buyId'],pandas_dfbuyp['count']);

df_game = dfgame.groupBy('teamLevel').count().toPandas()
display(pd)
plt.figure(figsize=(10,5))
plt.title('Top 10 highest purchasing Team level')
sns.barplot(df_game['teamLevel'],df_game['count']);

df_game

#df_game2= df_game.toPandas()
team_level = df_game.groupby("teamLevel").agg({'isHit': "sum"})

df_hits= dfgame.toPandas()
hits = df_hits.groupby("teamLevel").agg({'isHit': "sum"})
hits.plot.bar(title="Number of Hits per Team Levels",color='g')

hits

